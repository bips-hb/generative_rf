---
title: "SDV Benchmark (manually)"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
source("generative_ranger.R")
library(doParallel)
registerDoParallel()
```

````{r, include=FALSE}
my_generative_ranger <- function(x) {
  x_r <- py_to_r(x)
  generative_ranger(x_real = x_r, x_synth = NULL, n_new = nrow(x_r),
                    num.trees = 10, min.node.size = 50)
}
````

```{python}
def gen_rf(real_data):
  return r.my_generative_ranger(real_data)
```

# ------------------------------------------
# Replicate  Xu et al. 2020 benchmark study
# ------------------------------------------
# evaluating real data sets, table 5
# using a train/test split, table 4

````{python}
from sklearn.model_selection import train_test_split
from sdgym.datasets import load_dataset
from sdgym.datasets import load_tables
from sdv.tabular import CTGAN, CopulaGAN, GaussianCopula, TVAE
from sdv.metrics.tabular import BinaryDecisionTreeClassifier,BinaryAdaBoostClassifier,BinaryLogisticRegression,BinaryMLPClassifier
from sklearn.metrics import f1_score, accuracy_score, r2_score
````

````{python}
def Identity(): 
  pass 

def f1_none(*args):
  """
  should equal their f1_score
  """
  return f1_score(pos_label=0, *args)

def f1_macro(*args):
  return f1_score(average = 'macro', *args)

def f1_micro(*args):
  return f1_score(average = 'micro', *args)
````

```{python}

class benchmark:
    def __init__(self, dataset, test_size, model):
        """
        Args: 
        dataset: real dataset, spefiy one data set from Xu et al. 2020 Table 4,5
        test_size: train/test split as defined in Xu et al. 2020 Table 4
        model: model used to synthesize data, choose from Identity(), gen_rf, CTGAN(), GaussianCopula(), TVAE()
        
        """
        self.metadata = load_dataset(dataset)
        self.real_data = load_tables(self.metadata)[dataset]#[1:200] # comment out subset later
        self.real_data_train, self.real_data_test = train_test_split(self.real_data, test_size=test_size, random_state=2022)
        self.model = model
    def real_data_prediction(self):
        """ 
        TO DO: here will be the function that replicates table 5
        Returns: scores
        """
        pass 
    def synth_data(self):
        """ 
        Returns: synthesized data of size real_data_train
        """     
        if self.model == Identity():
            return self.real_data_train.copy()
        elif self.model == gen_rf:
            return gen_rf(self.real_data_train)
        else: 
            self.model.fit(self.real_data_train)
            return self.model.sample(self.real_data_train.shape[0])      
    def scores(self, list_of_classifiers, metric):
        """ 
        Args: 
        - list_of_classifiers: list of classifiers for the prediction task, subset of [BinaryDecisionTreeClassifier, BinaryAdaBoostClassifier,BinaryLogisticRegression, BinaryMLPClassifier]
        - metric: metric to use for score calculation,choose from f1_none, f1_micro, f1_macro, r2_score, accuracy_score
        Returns: scores
        """
        syn_dat = self.synth_data()
        res = {}
        for item in list_of_classifiers:
            res[item.__name__]=item.compute(self.real_data_test, syn_dat, target = "label", scorer = metric)
        return res
```

# example
```{python}
example = benchmark(dataset= 'adult', test_size=10/33, model=Identity())
example.scores(list_of_classifiers=[BinaryDecisionTreeClassifier,BinaryAdaBoostClassifier, BinaryLogisticRegression,BinaryMLPClassifier ], metric = f1_none)

```


# to do: list all combinations of data sets/classifiers
# to do: add pure prediction, def real_data_prediction(), using the same test set as with synthesized data (replicate table 4)

