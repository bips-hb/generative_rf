---
title: "SDV Benchmark (manually)"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
source("generative_ranger.R")
library(doParallel)
registerDoParallel()
```

````{r, include=FALSE}
my_generative_ranger <- function(x) {
  x_r <- py_to_r(x)
  generative_ranger(x_real = x_r, x_synth = NULL, n_new = nrow(x_r),
                    num.trees = 10, min.node.size = 50)
}
````

```{python}
def gen_rf(real_data):
  return r.my_generative_ranger(real_data)
```

# ------------------------------------------
# Replicate  Xu et al. 2020 benchmark study
# ------------------------------------------
# evaluating real data sets, table 5
# using a train/test split, table 4

````{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sdgym.datasets import load_dataset
from sdgym.datasets import load_tables
from sdv.tabular import CTGAN, CopulaGAN, GaussianCopula, TVAE
from sdv.metrics.tabular import BinaryDecisionTreeClassifier,BinaryAdaBoostClassifier,BinaryLogisticRegression,BinaryMLPClassifier
from sklearn.metrics import f1_score, accuracy_score, r2_score

#from sdgym.synthesizers import (CLBN, CopulaGAN, CTGAN, Identity, MedGAN, PrivBN, TableGAN, VEEGAN)

````

````{python}
def Identity(): 
    pass 

def f1_none(*args):
  """
  should equal their f1_score
  """
  return f1_score(average=None, *args)

def f1_macro(*args):
  return f1_score(average = 'macro', *args)

def f1_micro(*args):
  return f1_score(average = 'micro', *args)
````

```{python}

class benchmark:
    def __init__(self, dataset, test_size):
        """
        Args: 
        dataset: real dataset, spefiy one data set from Xu et al. 2020 Table 4,5
        test_size: train/test split as defined in Xu et al. 2020 Table 4
        
        """
        self.metadata = load_dataset(dataset)
        self.real_data = load_tables(self.metadata)[dataset][1:200] # comment out subset later
        self.real_data_train, self.real_data_test = train_test_split(self.real_data, test_size=test_size, random_state=2022)
    def synth_data(self, model):
        """ 
        Returns: synthesized data of size real_data_train
        """     
        if model == Identity():
            return self.real_data_train.copy()
        elif model == gen_rf:
            return gen_rf(self.real_data_train)
        else: 
            model.fit(data = self.real_data_train)
            return model.sample(self.real_data_train.shape[0])      
    def scores(self, list_of_classifiers, metric, dict_of_syn_models):
        """
        TO DO: list_of_syn_models = None as real data prediction, scores
        Args: 
        - list_of_classifiers: list of classifiers for the prediction task, subset of [BinaryDecisionTreeClassifier, BinaryAdaBoostClassifier,BinaryLogisticRegression, BinaryMLPClassifier]
        - metric: metric to use for score calculation,choose from f1_none, f1_micro, f1_macro, r2_score, accuracy_score
        Returns: scores
        """
        res = pd.DataFrame()
        syn_dat_res = pd.DataFrame()
        keys = list(dict_of_syn_models)
        for mod in range(len(dict_of_syn_models)):
            syn_dat = self.synth_data(model = dict_of_syn_models[keys[mod]])
            res = pd.DataFrame()
            for item in list_of_classifiers:
                scores = item.compute(self.real_data_test, syn_dat, target = "label", scorer = metric)
                new_metric = pd.DataFrame()
                for i in range(len(metric)): new_metric = pd.concat([new_metric, pd.DataFrame({metric[i].__name__ : [scores[i]] })], axis=1)
                res = res.append(pd.concat([pd.DataFrame({'model': keys[mod],'classifier': [item.__name__] }), new_metric], axis = 1))
            syn_dat_res = syn_dat_res.append(res)
        return syn_dat_res

```

# example
```{python}
ex = benchmark(dataset= 'adult', test_size=10/33)
result = ex.scores(list_of_classifiers=[BinaryDecisionTreeClassifier,BinaryAdaBoostClassifier,
BinaryLogisticRegression,BinaryMLPClassifier],metric=[accuracy_score, f1_none], dict_of_syn_models={"ID":Identity()})

```
```{python}
# models for synthetic data generation:
full_dict_of_syn_models = {"Identity":Identity(), "generative RF": gen_rf, "CTGAN":CTGAN()}
# CopulaGAN, GaussianCopula, TVAE not working yet, 
# for  sdgym.synthesizers (CLBN, CopulaGAN, CTGAN, Identity, MedGAN, PrivBN, TableGAN, VEEGAN) adapt .fit() procedure

#########################

# TO DO after solving the issues above, make code below more elegant --> bundle information on test_size, classifers, metrics etc. and loop over it

adult = benchmark(dataset= 'adult', test_size=10/(23+10))
result_adult = adult.scores(list_of_classifiers=[BinaryDecisionTreeClassifier,BinaryAdaBoostClassifier,
BinaryLogisticRegression,BinaryMLPClassifier],metric=[accuracy_score, f1_none], dict_of_syn_models= full_dict_of_syn_models)

census = benchmark(dataset='census', test_size=100/(200+100))
result_census = census.scores(list_of_classifiers=[BinaryDecisionTreeClassifier,BinaryAdaBoostClassifier,BinaryMLPClassifier],metric=[accuracy_score, f1_none], dict_of_syn_models= full_dict_of_syn_models)

covtype = benchmark(dataset='covtype', test_size=100/(481+100))
result_covtype = covtype.scores(list_of_classifiers=[BinaryDecisionTreeClassifier,BinaryMLPClassifier],metric=[accuracy_score, f1_micro, f1_macro], dict_of_syn_models= full_dict_of_syn_models)

credit = benchmark(dataset='credit', test_size=20/(264+20))
result_credit = credit.scores(list_of_classifiers=[BinaryDecisionTreeClassifier,BinaryAdaBoostClassifier,BinaryMLPClassifier],metric=[accuracy_score, f1_none], dict_of_syn_models= full_dict_of_syn_models)

intrusion = benchmark(dataset='intrusion', test_size=100/(394+100))
result_intrusion = intrusion.scores(list_of_classifiers=[BinaryDecisionTreeClassifier,BinaryMLPClassifier],metric=[accuracy_score, f1_micro, f1_macro], dict_of_syn_models= full_dict_of_syn_models)

mnist12 = benchmark(dataset='mnist12', test_size=10/(60+10))
result_mnist12 = mnist12.scores(list_of_classifiers=[BinaryDecisionTreeClassifier,BinaryLogisticRegression,BinaryMLPClassifier],metric=[accuracy_score, f1_micro, f1_macro], dict_of_syn_models= full_dict_of_syn_models)

mnist28 = benchmark(dataset='mnist28', test_size=10/(60+10))
result_mnist12 = mnist28.scores(list_of_classifiers=[BinaryDecisionTreeClassifier,BinaryLogisticRegression,BinaryMLPClassifier],metric=[accuracy_score, f1_micro, f1_macro], dict_of_syn_models= full_dict_of_syn_models)

````
